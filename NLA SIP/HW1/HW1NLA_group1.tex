\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{graphicx} % Required for \scalebox
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[extreme]{savetrees} % tighter margins
\usepackage{lmodern} % Use scalable Latin Modern fonts

\usepackage{bm}
\let\normalmu\mu
\let\normalSigma\Sigma
\renewcommand{\mu}{\bm{\normalmu}} % I want bold mu
\renewcommand{\Sigma}{\bm{\normalSigma}} % I want bold Sigma


\title{Baruch NLA SIP HW 1}
\author{group 1}

\begin{document}
\maketitle
\tableofcontents

\section{exercise 1 Daniel}
Let \(A = (a_{ij}) \in \mathbb{R}^{n\times n}\) and let \(D\) be diagonal with entries \(d_1, \dots, d_n\).
We use the Einstein summation convention (repeated indices are summed).

\[
    (AD)_{ij} = a_{ik} D_{kj} = a_{ik} \, d_k \, \delta_{kj} = a_{ij} \, d_j,
\]
\[
    (DA)_{ij} = D_{ik} a_{kj} = d_i \, \delta_{ik} \, a_{kj} = d_i \, a_{ij}.
\]

Thus
\[
    AD = DA \quad \Longleftrightarrow \quad (d_j - d_i)\, a_{ij} = 0
\]
for all \(i,j\) and all diagonal \(D\).

If \(A\) is diagonal, then \(a_{ij} = 0\) for \(i \neq j\), so \((d_j - d_i)a_{ij} = 0\) holds and \(AD = DA\).

Conversely, suppose \(AD = DA\) for every diagonal \(D\).
Fix \(i \neq j\) and choose \(D\) with \(d_i \neq d_j\) (e.g., \(d_i = 1,\ d_j = 0\), others arbitrary).
Then
\[
    (d_j - d_i)\, a_{ij} = 0 \quad \Longrightarrow \quad a_{ij} = 0.
\]
Since this holds for all \(i \neq j\), \(A\) is diagonal.

Therefore, \(AD = DA\) for any diagonal \(D\) if and only if \(A\) is diagonal.

\section{exercise 2 Daniel}
$M_2 M_3 M_4=M_1^{-1}$ because if we multiply both sides by $M_1$ from the left, we get $M_1 M_2 M_3 M_4 = I$, and the inverse matrix is unique.

Multiply the original statement by $M_1$ from the right and by $M_1^{-1}$ from the left to get $M_2 M_3 M_4 M_1 = I$. So we showed the first statement.
With this new truth, we can derive the inverse for $M_2$ with the same logic and get the 2nd statement,
from which we can derive the inverse for $M_3$, and confirm the 3rd statement, similarly for $M_4$ and for the last statement.

\section{exercise 3 Daniel}
Left-multiply by $I+A$ and right-multiply by $(I+A^{-1})$, which exists, as we can see from the problem statement:

\begin{align*}
    (I+A)(I+A)^{-1}(I+A^{-1})
     & = (I+A)\Bigl[I-(I+A^{-1})^{-1}\Bigr](I+A^{-1}) \\[2mm]
    I\,(I+A^{-1})
     & = (I+A)\Bigl[(I+A^{-1})-I\Bigr]                \\[2mm]
    I+A^{-1}
     & = (I+A)A^{-1}                                  \\
     & = I+A^{-1}.
\end{align*}
We made equivalent transformations throughout the process, the last equation is true, so the first equation must be true too.

\section{exercise 4 Hao Wang}

\begin{itemize}
    \item If \( A \) has rank 1, then all columns of \( A \) are scalar multiples of a single nonzero column vector \( v \). Let the \( i \)-th column of \( A \) be \( w_i v \), where \( w_i \) is a scalar. Then, \( A \) can be written as \( A = vw^T \), where \( w = [w_1, w_2, \dots, w_n]^T \).
    
    \item If \( A = vw^T \), then every column of \( A \) is a multiple of \( v \), and every row is a multiple of \( w^T \). Thus, the column space and row space of \( A \) are both 1-dimensional, so \( \text{rank}(A) = 1 \).
\end{itemize}

\section{exercise 5 Hao Wang}

Proof of $\det(I - xy^T) = 1 - y^T x$:

Let $x, y \in \mathbb{R}^n$. Consider the rank-1 matrix $xy^T$:

The matrix $xy^T$ has one non-zero eigenvalue: $\lambda = y^T x$ (since $xy^T x = x(y^T x) = (y^T x)x$) and $n-1$ zero eigenvalues (because $\text{rank}(xy^T) = 1$)

Therefore, $I - xy^T$ has eigenvalues: $1 - y^T x$ and $1$ with multiplicity $n-1$

The determinant is the product of eigenvalues, so we can get:
\[
\det(I - xy^T) = (1 - y^T x) \cdot 1^{n-1} = 1 - y^T x
\]

Based on this conclusion:
\begin{itemize}
    \item If \( I - xy^T \) is nonsingular, then \( \det(I - xy^T) \neq 0 \). Thus, \( 1 - y^T x \neq 0 \), which implies \( y^T x \neq 1 \).
    
    \item  If \( y^T x \neq 1 \), then \( \det(I - xy^T) = 1 - y^T x \neq 0 \), so \( I - xy^T \) is nonsingular.
\end{itemize}

\section{exercise 6 Daniel}
Wow! Multiply by $A+B$ from the left:
\[\begin{gathered}
        I + B{A^{ - 1}} + A{B^{ - 1}} + I = I\quad X: = B{A^{ - 1}} \hfill \\
        X + {X^{ - 1}} =  - I\quad /X \cdot  \hfill \\
        {X^2} + X + I = 0 \hfill \\
        \left( {{X^2} + X + I} \right)\left( {X - I} \right) = 0\left( {X - I} \right) \qquad \text{thanks, chatGPT}\hfill \\
        {X^3} - I = 0 \hfill \\
        {X^3} = I \Rightarrow \det \left( {{X^3}} \right) = 1 = {\left( {\frac{{\det \left( B \right)}}{{\det \left( A \right)}}} \right)^3}\quad {\text{if }}\det \left( A \right),\det \left( B \right) \in \mathbb{R} \Rightarrow \det \left( A \right) = \det \left( B \right) \hfill \\
    \end{gathered} \]

\section{exercise 7 Daniel}
% Covariance, variances, and correlation matrix
\[
    \Sigma =
    \begin{pmatrix}
        1      & -0.525 & 1.375  & -0.075 & -0.75  \\
        -0.525 & 4      & 0.1875 & 0.1875 & -0.675 \\
        1.375  & 0.1875 & 12.25  & 0.4375 & -1.875 \\
        -0.075 & 0.1875 & 0.4375 & 6.25   & 0.3    \\
        -0.75  & -0.675 & -1.875 & 0.3    & 4.41
    \end{pmatrix}.
\]

% Diagonal matrix of variances and the std-dev matrix
\[
    D=\mathrm{diag}(\Sigma)=
    \begin{pmatrix}
        1 & 0 & 0     & 0    & 0    \\
        0 & 4 & 0     & 0    & 0    \\
        0 & 0 & 12.25 & 0    & 0    \\
        0 & 0 & 0     & 6.25 & 0    \\
        0 & 0 & 0     & 0    & 4.41
    \end{pmatrix},
    \qquad
    S:=D^{1/2}=\mathrm{diag}(1,\,2,\,3.5,\,2.5,\,2.1).
\]

% Definition of the correlation matrix
\[
    \Omega \;=\; D^{-1/2}\,\Sigma\,D^{-1/2}
    \quad\Longleftrightarrow\quad
    \omega_{ij}=\frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}.
\]

% (Optional) exact fractional form
\[
    \Omega =
    \begin{pmatrix}
        1               & -\tfrac{21}{80} & \tfrac{11}{28}  & -\tfrac{3}{100} & -\tfrac{5}{14}  \\
        -\tfrac{21}{80} & 1               & \tfrac{3}{112}  & \tfrac{3}{80}   & -\tfrac{9}{56}  \\
        \tfrac{11}{28}  & \tfrac{3}{112}  & 1               & \tfrac{1}{20}   & -\tfrac{25}{98} \\
        -\tfrac{3}{100} & \tfrac{3}{80}   & \tfrac{1}{20}   & 1               & \tfrac{2}{35}   \\
        -\tfrac{5}{14}  & -\tfrac{9}{56}  & -\tfrac{25}{98} & \tfrac{2}{35}   & 1
    \end{pmatrix}.
\]

% Numerical result (6 d.p.)
\[
    \Omega \approx
    \begin{pmatrix}
        1         & -0.262500 & 0.392857  & -0.030000 & -0.357143 \\
        -0.262500 & 1         & 0.026786  & 0.037500  & -0.160714 \\
        0.392857  & 0.026786  & 1         & 0.050000  & -0.255102 \\
        -0.030000 & 0.037500  & 0.050000  & 1         & 0.057143  \\
        -0.357143 & -0.160714 & -0.255102 & 0.057143  & 1
    \end{pmatrix}.
\]


\section{exercise 8 Hao Wang}

\subsection*{(i) Covariance matrix with standard deviations (0.1, 0.2, 0.5, 1, 2):}

The covariance matrix \(\Sigma\) is given by:
\[
\Sigma = \begin{pmatrix}
0.01 & -0.005 & 0.0075 & -0.005 & -0.06 \\
-0.005 & 0.04 & -0.01 & -0.05 & 0.04 \\
0.0075 & -0.01 & 0.25 & 0.10 & 0.05 \\
-0.005 & -0.05 & 0.10 & 1 & 0.20 \\
-0.06 & 0.04 & 0.05 & 0.20 & 4
\end{pmatrix}
\]

\subsection*{(ii) Covariance matrix with standard deviations (2, 1, 0.5, 0.2, 0.1):}

The covariance matrix \(\Sigma\) is given by:
\[
\Sigma = \begin{pmatrix}
4 & -0.5 & 0.15 & -0.02 & -0.06 \\
-0.5 & 1 & -0.05 & -0.05 & 0.01 \\
0.15 & -0.05 & 0.25 & 0.02 & 0.0025 \\
-0.02 & -0.05 & 0.02 & 0.04 & 0.002 \\
-0.06 & 0.01 & 0.0025 & 0.002 & 0.01
\end{pmatrix}
\]


\section{exercise 9 Hao Wang}

Given the condition:
\[
\frac{1}{n}A^n = \frac{1}{n+1}A^{n+1} = \frac{1}{n+2}A^{n+2}
\]
Any eigenvalue \(\lambda\) of \(A\) must satisfy:
\[
\frac{1}{n}\lambda^n = \frac{1}{n+1}\lambda^{n+1} = \frac{1}{n+2}\lambda^{n+2}
\]
The only solution is \(\lambda = 0\). Thus, all eigenvalues of \(A\) must be 0.


\section{exercise 10 Hao Wang}

\subsection*{(i) Symmetric Positive Definite \(A\):}

For any positive integer \(m\), \(A^m\) is symmetric positive definite because:
\begin{itemize}
    \item \(A^m\) is symmetric.
    \item Eigenvalues of \(A^m\) are \(\lambda_i^m > 0\) given \(\lambda_i > 0\).
\end{itemize}

\subsection*{(ii) Symmetric Positive Semidefinite \(A\):}

For any positive integer \(m\), \(A^m\) is symmetric positive semidefinite because:
\begin{itemize}
    \item \(A^m\) is symmetric.
    \item Eigenvalues of \(A^m\) are \(\lambda_i^m \geq 0\) given \(\lambda_i \geq 0\).
\end{itemize}


\section{exercise 11 Hao Wang}

\subsection*{(i) \(M^t A M\) is Symmetric Positive Semidefinite:}

\begin{itemize}
    \item Symmetry: \((M^t A M)^t = M^t A M\).
    \item For any \(x\), \(x^t M^t A M x = (Mx)^t A (M x) \geq 0\).
\end{itemize}

\subsection*{(ii) \(M^t A M\) is Positive Definite if and only if Columns of \(M\) are Linearly Independent:}

\begin{itemize}
    \item If columns of \(M\) are independent, \(M x \neq 0\)  for \(x \neq 0\), then \(x^t M^t A M x > 0\).
    \item If \(x^t M^t A M x > 0\), then \(M x \neq 0\) for any \(x \neq 0\), thus columns of \(M\) are independent.
\end{itemize}

\section{exercise 12}
\section{exercise 13}
\section{exercise 14}
\section{exercise 15 Daniel}
The matrix is only of 4x4 and writing out all the equations would actually lead to a 4th order polynomial that can be reduced to a 2nd order polynomial.

We can also look up the Dan's LA Primer book for this example.

Another way is to get reminded by ChatGPT that we can reduce the problem to a linear, homogeneous, constant-coefficient recurrence relation. This looks like a pretty general approch so I go with this one.

Let the eigenvector be $\mathbf{p} = \begin{pmatrix} p_1 \\ p_2 \\ p_3 \\ p_4 \end{pmatrix}$, then we have the following system of equations:
\[ - {p_{j - 1}} + \left( {2 - \lambda } \right){p_j} - {p_{j + 1}} = 0\qquad \forall j \in \{ 1,2,3,4\} \quad {p_0} = {p_5} = 0\]
We solve it with the ansatz $p_j = r^j$ for some $r$. Plugging in gives us the characteristic equation:
\[ - r^{j-1} + (2 - \lambda) r^j - r^{j+1} = 0 \]
Dividing by $r^{j-1}$ (for $r \neq 0$) leads to:
\[ - 1 + (2 - \lambda) r - r^2 = 0 \]
Rearranging gives the standard form of a quadratic equation:
\[ r^2 - (2 - \lambda) r + 1 = 0 \]
Note that if $r$ is a root, then $r^{-1}$ is also a root, so we can write the general solution as
\[ p_j = A r^j + B r^{-j} \]
for some constants $A$ and $B$. $B=-A$ because $p_0 = 0$, so we can write
\[ p_j = A \left( r^j - r^{-j} \right) \]
At $p_5  =0$ we have
\[ 0 = A \left( r^5 - r^{-5} \right) \]
This implies that either $A = 0$ or $r^5 = r^{-5}$, which leads to $r^{10} = 1$. The solutions to this equation are the 10th roots of unity:
\[ r = e^{i\frac{2\pi k}{10}}, \quad k = 0, 1, \ldots, 4 \]
Note that $k>4$ result in the same solutions due to $B=-A$ and the periodicity of the exponential fucntion. From the quadratic equation, divided by $r$:
\[2-\lambda_k = e^{i\frac{2\pi k}{10}} + e^{-i\frac{2\pi k}{10}} = 2 \cos\left(\frac{2\pi k}{10}\right)\quad \]
So the matrix has the diagonal form of
\[
    \Lambda = \begin{pmatrix}
        2 - 2\cos\left(\frac{\pi}{5}\right) & 0                                    & 0                                    & 0                                    \\
        0                                   & 2 - 2\cos\left(\frac{2\pi}{5}\right) & 0                                    & 0                                    \\
        0                                   & 0                                    & 2 - 2\cos\left(\frac{3\pi}{5}\right) & 0                                    \\
        0                                   & 0                                    & 0                                    & 2 - 2\cos\left(\frac{4\pi}{5}\right)
    \end{pmatrix}
\]

\section{exercise 16 Hao Wang}

The given matrix \( J \) is an \( n \times n \) tridiagonal matrix with the following structure:
\[
J = \begin{pmatrix}
a & 0 & 0 & \cdots & 0 \\
b & a & 0 & \cdots & 0 \\
0 & b & a & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a
\end{pmatrix}
\]

\subsection*{Eigenvalues:}
The eigenvalues of \( J \) are all equal to \( a \). This is because \( J \) is a lower triangular matrix, and the eigenvalues of a triangular matrix are the entries on its main diagonal.

\subsection*{Eigenvectors:}
To find the eigenvectors corresponding to the eigenvalue \( a \), we solve:
\[
(J - a I) \mathbf{v} = 0
\]
This simplifies to:
\[
\begin{pmatrix}
0 & 0 & 0 & \cdots & 0 \\
b & 0 & 0 & \cdots & 0 \\
0 & b & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 0
\end{pmatrix}
\begin{pmatrix}
v_1 \\
v_2 \\
v_3 \\
\vdots \\
v_n
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0
\end{pmatrix}
\]

From this system, we deduce:
\[
v_1 = v_2 = \cdots = v_{n-1} = 0, \quad v_n \text{ is free.}
\]
Thus, the only eigenvector is:
\[
\mathbf{v} = \begin{pmatrix}
0 \\
0 \\
\vdots \\
0 \\
1
\end{pmatrix}
\]

\section{exercise 17 Daniel}
\[\begin{gathered}
        \Omega  = \rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}} \hfill \\
        \Omega v = \left( {\rho {\mathbf{1}}{{\mathbf{1}}^T} + \left( {\rho  - 1} \right){\mathbf{I}}} \right)v \hfill \\
        \Omega v = \rho {\mathbf{1}}\left( {{{\mathbf{1}}^T}v} \right) + \left( {1 - \rho } \right)v \Rightarrow \begin{array}{*{20}{c}}
            {{v_1} = {\mathbf{1}}\qquad {\lambda _1} = \rho n + 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \geqslant \frac{{ - 1}}{{n - 1}}}} \\
            {{\mathbf{1}^T}{v_{2,3, \ldots }} = 0\quad {\lambda _{2,3, \ldots }} = 1 - \rho  \geqslant 0 \Rightarrow \boxed{\rho  \leqslant 1}}
        \end{array} \hfill \\
    \end{gathered} \]

\section{exercise 18 Daniel}
The matrix can be written as
\[A= d{{\mathbf{I}}_{n \times n}} + {\mathbf{1}} \left( {1,2, \ldots ,n} \right)\]
The identity matrix has every vector as an eigenvector, and the rank-1 matrix has only 1 eigenvector $\mathbf{v}$ outside the diad's kernel, for which:
\[
    \hat\lambda \mathbf{v} = \left[{\mathbf{1}} \left( {1,2, \ldots ,n} \right) \right] \cdot \mathbf{v} = {\mathbf{1}} \cdot \left[ \left( {1,2, \ldots ,n} \right) \mathbf{v}\right] \Rightarrow \mathbf{v}_1 = \mathbf{1} \quad \hat\lambda_1 = \frac{n \cdot (n+1)}{2}
\]
And for the kernel:
\[\left[ {{\mathbf{1}}\left( {1,2, \ldots ,n} \right)} \right]{\mathbf{v}} = {\mathbf{1}}\underbrace {\left[ {\left( {1,2, \ldots ,n} \right){\mathbf{v}}} \right]}_0 \Rightarrow \hat\lambda _k = 0\quad \forall k \in \left\{ {2,3, \ldots ,n} \right\}\quad {{\mathbf{v}}_k} = {{\mathbf{e}}_1} - {{\mathbf{e}}_k}/k\]

So the eigenvalues for $A$ are
\begin{itemize}
    \item $\lambda_1 = d + \frac{n \cdot (n+1)}{2}$ and the eigenvector is $\mathbf{v}_1 = \mathbf{1}$.
    \item $\lambda_k = d$ and the eigenvectors are $\mathbf{v}_k = {\mathbf{e}}_1 - {{\mathbf{e}}_k}/k$ for $k \in \left\{2, 3, \ldots n\right\}$
\end{itemize}


\section{exercise 19 Daniel}
Let $v \neq0$, then $Qv \neq 0$ because $Q$ is orthogonal so it keeps the norm.

i) \[{v^t}{Q^t}AQv = {\left( {Qv} \right)^t}A\left( {Qv} \right) > 0\]
must hold because $A$ is spd, and $Qv$ is a non-zero vector. So $Q^tAQ$ is also spd.

ii) \[{v^t}{Q^t}AQv = {\left( {Qv} \right)^t}A\left( {Qv} \right) \geqslant 0\]
must hold because $A$ is spsd. So $Q^tAQ$ is also spsd.

\section{exercise 20 Daniel}
\[A = Q\Lambda {Q^t} \Rightarrow {A^{ - 1}} = {\left( {{Q^t}} \right)^{ - 1}}{\Lambda ^{ - 1}}{Q^{ - 1}}\]
$Q$ orthogonal, $Q^{-1} = Q^t$:
\[\begin{gathered}
        {A^{ - 1}} = Q{\Lambda ^{ - 1}}{Q^t} \hfill \\
        A{A^{ - 1}} = ?I = ?Q\Lambda {Q^t}Q{\Lambda ^{ - 1}}{Q^t} = Q\Lambda {\Lambda ^{ - 1}}{Q^t} = Q{Q^t} = I \hfill \\
    \end{gathered} \]
    
\section{exercise 21 Hao Wang}

Given the matrix:
\[
\Omega = \begin{pmatrix}
1 & 0.2 & 0.3 \\
0.2 & 1 & -0.2 \\
0.3 & -0.2 & 1
\end{pmatrix},
\]
we verify it is a correlation matrix by checking:
\begin{itemize}
    \item Symmetry: \(\Omega = \Omega^T\).
    \item Diagonal entries: All are 1.
    \item Positive semidefiniteness: All leading principal minors are non-negative.
\end{itemize}

The determinant is:
\[
\det(\Omega) = 0.806 \geq 0.
\]
Thus, \(\Omega\) is a valid correlation matrix.

The Cholesky factor \(U\) (upper triangular) satisfies \(\Omega = U^TU\). Computing \(U\):
\[
U \approx \begin{pmatrix}
1 & 0.2 & 0.3 \\
0 & 0.9798 & -0.2654 \\
0 & 0 & 0.9163
\end{pmatrix}
\]

\section{exercise 22 Hao Wang}

\subsection*{(i) Values of \(\rho\) for Valid Correlation Matrix:}

The matrix:
\[
\Omega = \begin{pmatrix}
1 & 0.4 & -0.4 \\
0.4 & 1 & \rho \\
-0.4 & \rho & 1
\end{pmatrix}
\]
must be positive semidefinite. The determinant condition yields:
\[
\rho^2 + 0.32\rho - 0.68 \leq 0 \implies \rho \in [-1, 0.68].
\]

\subsection*{(ii) Bounds for \(\rho = \text{corr}(X_1, X_2)\):}

Given correlations:
\[
\Omega = \begin{pmatrix}
1 & \rho & 0.4 & -0.2 \\
\rho & 1 & 0.2 & -0.3 \\
0.4 & 0.2 & 1 & -0.2 \\
-0.2 & -0.3 & -0.2 & 1
\end{pmatrix}
\]

We determine the valid range for $\rho$ by requiring all leading principal minors to be non-negative.

1. First-order Principal Minor
\[
\Delta_1 = 1 > 0 \quad \text{(Always satisfied)}
\]

2. Second-order Principal Minor
\[
\Delta_2 = \begin{vmatrix}
1 & \rho \\
\rho & 1 
\end{vmatrix} = 1 - \rho^2 \geq 0
\]
\[
\Rightarrow \rho \in [-1, 1]
\]

3. Third-order Principal Minor
\[
\Delta_3 = \begin{vmatrix}
1 & \rho & 0.4 \\
\rho & 1 & 0.2 \\
0.4 & 0.2 & 1
\end{vmatrix} 
= -\rho^2 + 0.16\rho + 0.8 \geq 0
\]
Solving the quadratic inequality:
\[
\rho^2 - 0.16\rho - 0.8 \leq 0 \Rightarrow \rho \in [-0.818, 0.978]
\]

4. Fourth-order Principal Minor (Full Determinant)
\begin{align*}
\Delta_4 &= \det(\bm{\Omega}) \\
&=\begin{vmatrix}
1-\rho^2 & 0.2-0.4\rho & -0.3+0.2\rho \\
0.2-0.4\rho & 0.84 & -0.12 \\
-0.3+0.2\rho & -0.12 & 0.96
\end{vmatrix} 
=-0.96\rho^2 + 0.216\rho + 0.6924 \geq 0
\end{align*}
Solving:
\[
0.96\rho^2 - 0.216\rho - 0.6924 \leq 0 \Rightarrow \rho \in [-0.744, 0.969]
\]

Intersection of all conditions:
\[
\rho \in [-0.744, 0.969]
\]

\section{exercise 23 Daniel}
The covariance matrix $\mathbf{\Sigma}$ is symmetric and positive definite,
so we can find an Cholesky decomposition,
using which we can define the random vector $\mathbf{X}$ that has the right properties.

\[{\mathbf{\Sigma }} = {\mathbf{L}}{{\mathbf{L}}^T}\qquad {\mathbf{LZ}} = {\mathbf{X}} \Rightarrow Var\left( {\mathbf{X}} \right) = {\mathbf{L}}{{\mathbf{L}}^T} = {\mathbf{\Sigma }}\]

\[{\mathbf{L}} = \left( {\begin{array}{*{20}{c}}
            a & 0 \\
            d & c
        \end{array}} \right) \Rightarrow \begin{array}{*{20}{c}}
        {{a^2} = 9} \\
        {ab =  - 4} \\
        {{b^2} + {c^2} = 16}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {a = 3}      \\
        {b =  - 4/3} \\
        {c = 8\sqrt 2 /3}
    \end{array} \Rightarrow \begin{array}{*{20}{c}}
        {{X_1} = 3{Z_1}} \\
        {{X_2} =  - 4{Z_1}/3 + 8\sqrt 2 {Z_2}/3}
    \end{array}
\]

\section{exercise 24}
Use notation
\[\left( {\begin{array}{*{20}{c}}
            {{X_1}} \\
            {{X_2}} \\
            {{X_3}}
        \end{array}} \right) = {\mathbf{X}}\sim\mathcal{N}\left( {{\mathbf{\mu }},{\mathbf{\Sigma }}} \right)\]

Let ${\mathbf{\hat X}} = {\mathbf{X}} - {\mathbf{\mu }}$,
then $Var\left( {{\mathbf{\hat X}}} \right) = Var\left( {\mathbf{X}} \right) = {\mathbf{\Sigma }}$.
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {{{\mathbf{a}}^T}\left( {{\mathbf{\mu }} + {\mathbf{\hat X}}} \right) > 0} \right) = P\left( {\underbrace {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}}_Y} > 0 \right)\]

What is the distribution of $Y$?
\[E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = E\left( {{{\mathbf{a}}^T}{\mathbf{\mu }}} \right) + E\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}{\mathbf{\mu }} = -4\]
\[Var\left( {{{\mathbf{a}}^T}{\mathbf{\mu }} + {{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = Var\left( {{{\mathbf{a}}^T}{\mathbf{\hat X}}} \right) = {{\mathbf{a}}^T}Var\left( {{\mathbf{\hat X}}} \right){\mathbf{a}} = {{\mathbf{a}}^T}{\mathbf{\Sigma a}} = 32\]
We also know that $Y$ is a linear combination of normally distributed variables, so it is also normally distributed:
\[Y\sim \mathcal N({{\mathbf{a}}^T}{\mathbf{\mu }}, {{\mathbf{a}}^T}{\mathbf{\Sigma a}}) = \mathcal N(-4, 32)\]
and
\[Z=\frac{Y - E(Y)}{\sqrt{Var(Y)}} = \frac{Y + 4}{\sqrt{32}} \sim \mathcal N(0, 1)\]
\[P\left( {{{\mathbf{a}}^T}{\mathbf{X}} > 0} \right) = P\left( {Y > 0} \right) = P\left( {\frac{{Y + 4}}{{\sqrt {32} }} > \frac{4}{{\sqrt {32} }}} \right) = P\left( {Z > \frac{4}{{\sqrt {32} }}} \right) \approx 0.24\]
\end{document}
